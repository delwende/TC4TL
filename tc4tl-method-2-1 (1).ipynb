{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Machine learning model development and deployment for the task of Covid-19 contact tracing**\n**My contribution in the paper: **","metadata":{}},{"cell_type":"code","source":"import statistics\nfrom sklearn.model_selection import train_test_split\nimport os\nimport random\nimport math\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch import optim\nimport time\nimport matplotlib.pyplot as plt\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-03T07:42:32.399577Z","iopub.execute_input":"2021-11-03T07:42:32.400391Z","iopub.status.idle":"2021-11-03T07:42:32.407455Z","shell.execute_reply.started":"2021-11-03T07:42:32.400332Z","shell.execute_reply":"2021-11-03T07:42:32.406766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'You are using {device}')","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:32.409303Z","iopub.execute_input":"2021-11-03T07:42:32.409727Z","iopub.status.idle":"2021-11-03T07:42:32.426177Z","shell.execute_reply.started":"2021-11-03T07:42:32.409679Z","shell.execute_reply":"2021-11-03T07:42:32.425333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir=!tar xzvf ../input/datanist/tc4tl_training_data_v1.tgz\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:32.427951Z","iopub.execute_input":"2021-11-03T07:42:32.428567Z","iopub.status.idle":"2021-11-03T07:42:45.119806Z","shell.execute_reply.started":"2021-11-03T07:42:32.428532Z","shell.execute_reply":"2021-11-03T07:42:45.118815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INTERVAL_LENGTH=4\nNUM_READINGS_PER_INTERVAL=150","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:45.122192Z","iopub.execute_input":"2021-11-03T07:42:45.122449Z","iopub.status.idle":"2021-11-03T07:42:45.130354Z","shell.execute_reply.started":"2021-11-03T07:42:45.122422Z","shell.execute_reply":"2021-11-03T07:42:45.129455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrainset_path=train_dir[0]\ntrainkey_path=train_dir[len(train_dir)-1]\n\ntest_key_path=\"../input/validation/validation/docs/tc4tl_test_metadata.tsv\"\ntest_data_path=\"../input/validation/validation/data/test\"\nval_key_path=\"../input/validation/validation/docs/tc4tl_dev_key.tsv\"\nval_data_path=\"../input/validation/validation/data/dev\"","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:45.131603Z","iopub.execute_input":"2021-11-03T07:42:45.132406Z","iopub.status.idle":"2021-11-03T07:42:45.522136Z","shell.execute_reply.started":"2021-11-03T07:42:45.132362Z","shell.execute_reply":"2021-11-03T07:42:45.521159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read key to dataframe\ndf_train_key = pd.read_csv(trainkey_path, sep='\\t', index_col=\"fileid\")\ndf_train_key.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:42:45.523853Z","iopub.execute_input":"2021-11-03T07:42:45.524415Z","iopub.status.idle":"2021-11-03T07:42:45.562157Z","shell.execute_reply.started":"2021-11-03T07:42:45.524376Z","shell.execute_reply":"2021-11-03T07:42:45.561284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val_key=pd.read_csv(val_key_path, sep='\\t', index_col=\"fileid\")\ndf_test_key=pd.read_csv(test_key_path, sep='\\t', index_col=\"fileid\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T08:36:08.331482Z","iopub.execute_input":"2021-11-03T08:36:08.331966Z","iopub.status.idle":"2021-11-03T08:36:08.387355Z","shell.execute_reply.started":"2021-11-03T08:36:08.331928Z","shell.execute_reply":"2021-11-03T08:36:08.386656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_key_fined=df_train_key[df_train_key[\"coarse_grain\"]=='N']\ndf_train_key_coarse=df_train_key[df_train_key[\"coarse_grain\"]=='Y']\n\ndf_val_key_fined=df_val_key[df_val_key[\"coarse_grain\"]=='N']\ndf_val_key_coarse=df_val_key[df_val_key[\"coarse_grain\"]=='Y']\n\ndf_test_key_fined=df_test_key[df_test_key[\"coarse_grain\"]=='N']\ndf_test_key_coarse=df_test_key[df_test_key[\"coarse_grain\"]=='Y']","metadata":{"execution":{"iopub.status.busy":"2021-11-03T08:36:09.529099Z","iopub.execute_input":"2021-11-03T08:36:09.529355Z","iopub.status.idle":"2021-11-03T08:36:09.550894Z","shell.execute_reply.started":"2021-11-03T08:36:09.529326Z","shell.execute_reply":"2021-11-03T08:36:09.550106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fined_file_list=list(df_train_key_fined.index)\ncoarse_file_list=list(df_train_key_coarse.index)\n\nval_fined_file_list=list(df_val_key_fined.index)\nval_coarse_file_list=list(df_val_key_coarse.index)\n\ntest_fined_file_list=list(df_test_key_fined.index)\ntest_coarse_file_list=list(df_test_key_coarse.index)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T08:36:10.623259Z","iopub.execute_input":"2021-11-03T08:36:10.623963Z","iopub.status.idle":"2021-11-03T08:36:10.634271Z","shell.execute_reply.started":"2021-11-03T08:36:10.623922Z","shell.execute_reply":"2021-11-03T08:36:10.631787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_metadata(key_path, data_path,isFineOrCoarse,isCF):\n# first pass to find the various values for the categorical features to use for one-hot encoding\n    fixed_variables_possible_values = [set() for _ in range(9)]\n    \n    class_labels_possible_values = set()\n    for file_id in os.listdir(data_path):\n        if file_id in isFineOrCoarse:\n            if file_id.startswith(\".\"):\n                # there are some weird extra files starting with .\n                continue\n            with open(os.path.join(data_path, file_id), 'r', errors=\"ignore\") as data_file:\n                for index in range(7):\n                    value = data_file.readline().strip().split(\",\")[1]\n                    fixed_variables_possible_values[index].add(value)\n    with open(key_path, 'r', errors=\"ignore\") as key_file:\n        key_file.readline()  # skip header\n        for line in key_file:\n            record = line.split(\"\\t\")\n            \n            if record[4].lstrip().rstrip() ==isCF:\n                \n                transmitter_position, receiver_position = record[1].split(\"_\")\n                if len(record) == 5:\n                    # has labels\n\n                    class_labels_possible_values.add(float(record[2]))\n                fixed_variables_possible_values[7].add(transmitter_position)\n                fixed_variables_possible_values[8].add(receiver_position)\n    for i in range(3,9):\n        fixed_variables_possible_values[i].add(\"unknown\")\n    fixed_variables_possible_values = [list(fixed_variable_possible_values) for fixed_variable_possible_values in fixed_variables_possible_values]\n    class_labels_possible_values = list(class_labels_possible_values)\n    return fixed_variables_possible_values,class_labels_possible_values","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:50:07.587251Z","iopub.execute_input":"2021-11-03T07:50:07.587913Z","iopub.status.idle":"2021-11-03T07:50:07.598447Z","shell.execute_reply.started":"2021-11-03T07:50:07.587872Z","shell.execute_reply":"2021-11-03T07:50:07.597371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_averages(list_2d):\n    cell_total = list()\n    row_totals = dict()\n    column_totals = dict()\n    for row_idx, row in enumerate(list_2d):\n        for cell_idx, cell in enumerate(row):\n            # is cell a number?\n            if type(cell) in [int, float, complex]:\n                cell_total.append(cell)                \n                if row_idx in row_totals:\n                    row_totals[row_idx].append(cell)\n                else:\n                    row_totals[row_idx] = [cell]\n                if cell_idx in column_totals:\n                    column_totals[cell_idx].append(cell)\n                else:\n                    column_totals[cell_idx] = [cell]\n    per_row_avg = [sum(row_totals[row_idx]) / len(row_totals[row_idx]) for row_idx in row_totals]\n    per_col_avg = [sum(column_totals[col_idx]) / len(column_totals[col_idx]) for col_idx in column_totals]\n    row_avg = sum(per_row_avg) / len(per_row_avg)\n    col_avg = sum(per_col_avg) / len(per_col_avg)\n    return {'cell_average': sum(cell_total) / len(cell_total),\n            'per_row_average': per_row_avg,\n            'per_column_average': per_col_avg,\n            'row_average': row_avg,\n            'column_average': col_avg}","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:50:08.604093Z","iopub.execute_input":"2021-11-03T07:50:08.604868Z","iopub.status.idle":"2021-11-03T07:50:08.615353Z","shell.execute_reply.started":"2021-11-03T07:50:08.604819Z","shell.execute_reply":"2021-11-03T07:50:08.614615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_distance(TX, RSSI, N):\n    result=10**((TX-RSSI)/(10-N))\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:50:09.277129Z","iopub.execute_input":"2021-11-03T07:50:09.277646Z","iopub.status.idle":"2021-11-03T07:50:09.282413Z","shell.execute_reply.started":"2021-11-03T07:50:09.27761Z","shell.execute_reply":"2021-11-03T07:50:09.281266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(key_path, data_path,fixed_variables_possible_values,class_labels_possible_values,isFineOrCoarse,isTest):\n    X = list()  # each intervals index in this list gives you the input features\n    y = list()  # each intervals index in this list gives you the label\n    interval_to_file = list()\n    check=set()# this is used to output predictions. Each intervals index in this list gives you the file that the interval was from\n    with open(key_path, 'r') as key_file:\n        key_file.readline()  # skip header\n        for line in key_file:\n            record = line.split(\"\\t\")\n            file_id = record[0]\n            if file_id in isFineOrCoarse:\n                TX=-54\n                N=2.1\n                if isTest and record[3].lstrip().rstrip() ==\"Y\":\n                    TX=-52\n                    N=2.6\n                elif not isTest and  record[4].lstrip().rstrip() ==\"Y\":\n                    TX=-52\n                    N=2.6\n                with open(os.path.join(data_path, file_id)) as data_file:\n                    # fixed variables tx_device, tx_power, rx_device, tx_carry, rx_carry, rx_pose, tx_pose,\n                    #                              transmitter_position, receiver_position\n                    txpower=7\n                    fixed_variables=[]\n                    for i in range(7):\n                        val=data_file.readline().strip().split(\",\")[1]\n                        if i==1 and (val==\"Unknown\" or val==\"unknown\"):\n                            val=7\n                        if i==1:\n                            txpower=int(val)\n                        fixed_variables.append(val)\n                    #fixed_variables = [data_file.readline().strip().split(\",\")[1] for _ in range(7)]\n                    fixed_variables.extend(record[1].split(\"_\"))\n                    fixed_part = list()\n                    #ONE HOT ENCODING\n                    for variable, key in zip(fixed_variables, fixed_variables_possible_values):\n                        fixed_part.extend([int(possible_value == variable) for possible_value in key])\n                    #print(\"Loading file {} with fixed variables of {}\".format(file_id, fixed_part))\n                    interval_start_time = 0\n                    interval_data = list()\n                    num_intervals = 0\n                    reading_count = 0\n                    previous_value = {\n                        \"Bluetooth\": (0,),\n                        \"Accelerometer\": (0,0,0),\n                        \"Gyroscope\": (0,0,0),\n                        \"path_loss\":(0,),\n                        \"distance\":(0,)\n                    }\n                    for line in data_file:\n                        reading = line.strip().split(\",\")\n                        curr_time = float(reading[0])\n                        if (curr_time - interval_start_time) > INTERVAL_LENGTH:\n                            if reading_count > NUM_READINGS_PER_INTERVAL:\n                                # randomly remove readings\n                                for _ in range(reading_count - NUM_READINGS_PER_INTERVAL):\n                                    interval_data.pop(math.floor(random.random() * len(interval_data)))\n                            else:\n                                # randomly duplicate readings\n                                # todo: try other methods such as averaging\n                                for _ in range(NUM_READINGS_PER_INTERVAL - reading_count):\n                                    random_index = math.floor(random.random() * len(interval_data))\n                                    cal_avg=calculate_averages(interval_data)\n                                    interval_data.insert(random_index, cal_avg[\"per_column_average\"])\n                                    #print(interval_data)\n                            X.append(interval_data)\n                            num_intervals += 1\n\n                            # reset values\n                            interval_start_time = curr_time\n                            reading_count = 0\n                            interval_data = list()\n                            previous_value = {\n                                \"Bluetooth\": (0,),\n                                \"Accelerometer\": (0,0,0),\n                                \"Gyroscope\": (0,0,0),\n                                \"path_loss\":(0,),\n                                \"distance\":(0,)\n                            }\n                        type = reading[1]\n                        if type in {\"Pedometer\", \"Activity\",\"Heading\", \"Altitude\", \"Attitude\",\"Gravity\",\"Magnetic-field\"}: #, \"Heading\", \"Altitude\", \"Attitude\", \"Gravity\"}:\n                            continue\n                        elif type == \"Bluetooth\":\n                            previous_value[type] = (float(reading[2]), )\n                            previous_value[\"path_loss\"]=(txpower-41-float(reading[2]),)\n                            previous_value[\"distance\"]=(compute_distance(TX,float(reading[2]),N),)\n                        else:\n                            check.add(type)\n                            previous_value[type] = (float(reading[2]), float(reading[3]), float(reading[4]))\n                        # combine the values in previous_value into one giant list\n                        # Uses 0 as angle for nist data\n\n                        interval_data.append( [reading for value in previous_value.values() for reading in value]  + fixed_part)\n                        reading_count += 1\n                    # the last interval needs to be added manually\n                    if reading_count > NUM_READINGS_PER_INTERVAL:\n                        # randomly remove readings\n                        for i in range(reading_count - NUM_READINGS_PER_INTERVAL):\n                            interval_data.pop(math.floor(random.random() * len(interval_data)))\n                    else:\n                        # randomly duplicate readings\n                        # todo: try other methods such as averaging\n                        for i in range(NUM_READINGS_PER_INTERVAL - reading_count):\n                            random_index = math.floor(random.random() * len(interval_data))\n                            interval_data.insert(random_index, interval_data[random_index])\n                    X.append(interval_data)\n                    num_intervals += 1\n\n                if len(record) == 5:\n                    # this means this file has labels\n                    distance = float(record[2])\n                    label = torch.zeros(INTERVAL_LENGTH)\n                    label[class_labels_possible_values.index(distance)] = 1\n                    for _ in range(num_intervals):\n                        y.append(label)\n\n                for _ in range(num_intervals):\n                    interval_to_file.append(file_id)\n    \n    return [torch.Tensor(interval) for interval in X], y, class_labels_possible_values, interval_to_file","metadata":{"execution":{"iopub.status.busy":"2021-11-03T09:01:09.157323Z","iopub.execute_input":"2021-11-03T09:01:09.157812Z","iopub.status.idle":"2021-11-03T09:01:09.18391Z","shell.execute_reply.started":"2021-11-03T09:01:09.157777Z","shell.execute_reply":"2021-11-03T09:01:09.183218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fixed_variables_possible_values,class_labels_possible_values=read_metadata( trainkey_path,trainset_path,coarse_file_list,\"Y\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:09:59.205644Z","iopub.execute_input":"2021-11-03T10:09:59.206214Z","iopub.status.idle":"2021-11-03T10:10:01.437508Z","shell.execute_reply.started":"2021-11-03T10:09:59.206176Z","shell.execute_reply":"2021-11-03T10:10:01.436769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_labels_possible_values","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:10:01.715256Z","iopub.execute_input":"2021-11-03T10:10:01.715888Z","iopub.status.idle":"2021-11-03T10:10:01.723434Z","shell.execute_reply.started":"2021-11-03T10:10:01.715846Z","shell.execute_reply":"2021-11-03T10:10:01.722711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X, train_y, train_labels_to_distance, train_intervals_to_file = load_data( trainkey_path,trainset_path,fixed_variables_possible_values,class_labels_possible_values,coarse_file_list,False)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:10:27.952631Z","iopub.execute_input":"2021-11-03T10:10:27.953143Z","iopub.status.idle":"2021-11-03T10:57:19.619203Z","shell.execute_reply.started":"2021-11-03T10:10:27.953106Z","shell.execute_reply":"2021-11-03T10:57:19.616538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest_X, _, test_labels_to_distance, test_intervals_to_file = load_data(test_key_path, test_data_path,fixed_variables_possible_values,class_labels_possible_values,test_coarse_file_list,True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T10:57:19.621265Z","iopub.execute_input":"2021-11-03T10:57:19.621502Z","iopub.status.idle":"2021-11-03T11:01:07.826731Z","shell.execute_reply.started":"2021-11-03T10:57:19.621469Z","shell.execute_reply":"2021-11-03T11:01:07.825899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_X, val_y, labels_to_distance, dev_interval_to_file = load_data(val_key_path, val_data_path,fixed_variables_possible_values,class_labels_possible_values,val_coarse_file_list,False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:07.830514Z","iopub.execute_input":"2021-11-03T11:01:07.830714Z","iopub.status.idle":"2021-11-03T11:01:31.059133Z","shell.execute_reply.started":"2021-11-03T11:01:07.830689Z","shell.execute_reply":"2021-11-03T11:01:31.05837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_data_loader = torch.utils.data.DataLoader(list(zip(train_X, train_y)), batch_size=128, drop_last=True)\nval_data_loader = torch.utils.data.DataLoader(list(zip(val_X, val_y)), batch_size=128,drop_last=True)\ntest_data_loader= torch.utils.data.DataLoader(list(zip(test_X, _)), batch_size=128,drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:31.061105Z","iopub.execute_input":"2021-11-03T11:01:31.061366Z","iopub.status.idle":"2021-11-03T11:01:31.437237Z","shell.execute_reply.started":"2021-11-03T11:01:31.061332Z","shell.execute_reply":"2021-11-03T11:01:31.436504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CSELoss(predictions, targets, epsilon=1e-12):\n  \"\"\"\n  cross entropy loss  \n  \"\"\"\n\n  predictions = torch.clamp(predictions, epsilon, 1. - epsilon)\n  N = predictions.shape[0]\n  ce = -torch.sum(targets*torch.log(predictions+1e-9))/N\n  return ce","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:31.438779Z","iopub.execute_input":"2021-11-03T11:01:31.440082Z","iopub.status.idle":"2021-11-03T11:01:31.446645Z","shell.execute_reply.started":"2021-11-03T11:01:31.440042Z","shell.execute_reply":"2021-11-03T11:01:31.445805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_ForecastNet_pooled(nn.Module):\n    def __init__(self,input_size, len_timestamp, hidden_size, output_size, kernel_size):\n        super(CNN_ForecastNet_pooled,self).__init__()\n\n        self.input_size= input_size\n        self.len_timestamp= len_timestamp\n        self.hidden_size= hidden_size\n        self.kernel_size= kernel_size\n        self.output_size= output_size\n        self.padding = kernel_size // 2\n        \n        self.conv1 = nn.Conv1d(self.input_size, self.hidden_size, kernel_size=3, padding=self.padding)\n        self.conv2 = nn.Conv1d(self.hidden_size, self.hidden_size // 2, kernel_size=3, padding=self.padding)\n        self.conv3 = nn.Conv1d(self.hidden_size // 2, self.hidden_size // 2, kernel_size=3, padding=self.padding)\n        #self.maxpool1d = nn.MaxPool1d(kernel_size=3)\n        self.conv_outdim = (self.len_timestamp //16)\n        self.linear_input_size = self.hidden_size * self.conv_outdim\n        self.fc1 = nn.Linear(self.linear_input_size, 128)\n        self.fc2 = nn.Linear(128, self.output_size)\n        self.softmax = nn.Softmax()\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout()\n\n    def forward(self,x):\n\n        x = x.permute(0,2,1)\n        out = nn.functional.max_pool1d(self.relu(self.conv1(x)), 2)\n        out1 = out\n        out= self.dropout(out)\n        out = nn.functional.max_pool1d(self.relu(self.conv2(out)), 2)\n        out= self.dropout(out)\n        out = nn.functional.max_pool1d(self.relu(self.conv3(out)), 2)\n        out= self.dropout(out)\n        out= out.view(out.size(0), -1)\n        out = self.relu(self.fc1(out))\n        out = self.fc2(out)\n        y_pred = self.softmax(out)\n\n        return y_pred\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:31.448117Z","iopub.execute_input":"2021-11-03T11:01:31.448405Z","iopub.status.idle":"2021-11-03T11:01:31.46396Z","shell.execute_reply.started":"2021-11-03T11:01:31.448358Z","shell.execute_reply":"2021-11-03T11:01:31.462953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, val_data_loader, best_acc, save=True):\n  model.eval()\n  predict_dist = {0:0,1:0,2:0,3:0}\n  label_dist = {0:0,1:0,2:0,3:0}\n  with torch.no_grad():\n      test_total_loss = list()\n      test_total_acc = list()\n      test_total_tc_acc = list()\n      for idx, batch in enumerate(val_data_loader):\n          input = batch[0]\n          label = torch.max(batch[1],axis=1)[1]\n          for lab in label:\n            label_dist[int(lab)] += 1\n          # if idx % 100 == 0:\n          #   print(\"label {}\".format(label))\n          prediction = model(input) #, batch_size=len(label))\n          loss = loss_fn(prediction, batch[1])\n          prediction = torch.max(prediction, 1)[1].view(label.size())\n          for pred in prediction:\n            predict_dist[int(pred)] += 1\n          # if idx % 100 == 0:\n          #   print(\"pred {}\".format(prediction))\n          num_corrects = (prediction == label).float().sum()\n          acc = 100.0 * num_corrects / len(label)\n          prediction = ((prediction == 0) + (prediction == 3).float())\n          label = ((label == 0) + (label == 3).float())\n          num_tc_corrects = (label == prediction).float().sum()\n          tc_acc = 100.0 * num_tc_corrects / len(label)\n          test_total_loss.extend([loss.item()] * len(label))\n          test_total_acc.extend([acc.item()] * len(label))\n          test_total_tc_acc.extend([tc_acc.item()] * len(label))\n          # if idx % 100 == 0:\n          #   print(\"acc {}\".format(acc))\n      \n      curr_acc = statistics.mean(test_total_acc)\n      if best_acc < curr_acc and save:\n        torch.save(model.state_dict(), MODEL_PATH)\n        best_acc = curr_acc\n      #print(predict_dist)\n      #print(label_dist)\n      return statistics.mean(test_total_loss), statistics.mean(test_total_acc), statistics.mean(test_total_tc_acc), best_acc\nimport json\n\ndef output_predictions(model, X, labels_to_distance, intervals_to_file, output_path):\n  model.eval()\n  with torch.no_grad():\n      with open(output_path, \"w\") as f:\n        f.write(\"fileid\\tdistance\\n\")\n        file_to_interval_pred = dict()\n        i = 0\n        for tensor, file_id in zip(X, intervals_to_file):\n            i+=1\n            #print(i)\n            if file_id not in file_to_interval_pred:\n              file_to_interval_pred[file_id] = list()            \n            input = tensor.view(1, tensor.shape[0], tensor.shape[1])\n            prediction = model(input)#, batch_size=1)\n            prediction = labels_to_distance[torch.max(prediction, 1)[1]]\n            file_to_interval_pred[file_id].append(str(prediction))\n        file_to_pred = [file_id + \"\\t\" + max(set(preds), key=preds.count) for file_id, preds in file_to_interval_pred.items()]\n        #print(len(file_to_pred))\n        file_to_pred.sort()\n        f.write(\"\\n\".join(file_to_pred))\n  return","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:31.465498Z","iopub.execute_input":"2021-11-03T11:01:31.465987Z","iopub.status.idle":"2021-11-03T11:01:31.485425Z","shell.execute_reply.started":"2021-11-03T11:01:31.465949Z","shell.execute_reply":"2021-11-03T11:01:31.484688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nEXP_NAME = \"CNN_ForecastNet_pooled_coarse\"\nMODEL_PATH = \"./model_epochs_100_{}.pth\".format(EXP_NAME)\nlabels_to_distance = [1.2, 3.0, 4.5,1.8]\nnum_features= 55\n\nmodel =CNN_ForecastNet_pooled(input_size=num_features,len_timestamp =NUM_READINGS_PER_INTERVAL, hidden_size=64, output_size=len(labels_to_distance), kernel_size=3)\nprint(model)\nbest_acc = 0\ntotal_loss = []  # for plotting\ntotal_test_loss = []\nloss_fn = CSELoss\ntrain_start_time = time.time()\nepoch_num=100\n\nfor epoch in range(epoch_num):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    total_epoch_tc_acc = 0\n    # optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5,weight_decay=1e-4)\n    steps = 0\n    model.train()\n    for idx, batch in enumerate(train_data_loader):\n        input = batch[0]\n        label = torch.max(batch[1], axis=1)[1]\n        optim.zero_grad()\n        prediction = model(input)#, batch_size=len(label))\n        loss = loss_fn(prediction, batch[1])\n        prediction = torch.max(prediction, 1)[1].view(label.size())\n\n        num_corrects = (prediction == label).float().sum()\n        acc = 100.0 * num_corrects / len(label)\n        prediction = ((prediction == 0) + (prediction == 3).float())\n        label = ((label == 0) + (label == 3).float())\n        num_tc_corrects = (label == prediction).float().sum()\n        tc_acc = 100.0 * num_tc_corrects / len(label)\n        loss.backward()\n        optim.step()\n        steps += 1\n        total_epoch_loss += loss.item()\n        total_epoch_acc += acc.item()\n        total_epoch_tc_acc += tc_acc.item()\n    # eval\n    print(\"before eval\")\n    loss, acc, tc_acc, best_acc = eval_model(model, val_data_loader, best_acc)\n    print(\"Testing Loss: {}, AVG: {}, TC4TL or Not AVG: {}\".format(loss, acc, tc_acc))\n    total_loss.append((epoch, total_epoch_loss/steps))\n    total_test_loss.append((epoch, loss))\n    print (f'Epoch: {epoch+1}, Training Loss: {total_epoch_loss/steps:.4f}, Training Accuracy: {total_epoch_acc/steps: .2f}% TC4TL or Not ACC: {total_epoch_tc_acc/steps: .2f}%')\nprint(\"finished training, took {} seconds\".format(time.time() - train_start_time))\nplt.scatter(*zip(*total_loss))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"loss\")\nplt.show()\nplt.scatter(*zip(*total_test_loss))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"test loss\")\nplt.show()\n\n# eval\nloss, acc, tc_acc, best_acc = eval_model(model, val_data_loader, best_acc)\nprint(\"Testing Loss: {}, AVG: {}, TC4TL or Not AVG: {}\".format(loss, acc, tc_acc))\nprint(best_acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:01:31.486922Z","iopub.execute_input":"2021-11-03T11:01:31.487181Z","iopub.status.idle":"2021-11-03T11:40:16.533918Z","shell.execute_reply.started":"2021-11-03T11:01:31.487147Z","shell.execute_reply":"2021-11-03T11:40:16.533148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(*zip(*total_loss))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"loss\")\nplt.show()\nplt.scatter(*zip(*total_test_loss))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"test loss\")\nplt.show()\n\n# eval\nloss, acc, tc_acc, best_acc = eval_model(model, val_data_loader, best_acc)\nprint(\"Testing Loss: {}, AVG: {}, TC4TL or Not AVG: {}\".format(loss, acc, tc_acc))\nprint(best_acc)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:40:16.536516Z","iopub.execute_input":"2021-11-03T11:40:16.536802Z","iopub.status.idle":"2021-11-03T11:40:17.71018Z","shell.execute_reply.started":"2021-11-03T11:40:16.536758Z","shell.execute_reply":"2021-11-03T11:40:17.709414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST ON VAL SECTION OF DEV DATA\nEXP_NAME = \"CNN_ForecastNet_pooled_coarse\"\nMODEL_PATH = \"./model_epochs_100_{}.pth\".format(EXP_NAME)\nlabels_to_distance = [1.2, 3.0, 4.5,1.8]\nnum_features = 55\nbest_acc=0\n\nval_data_loader = torch.utils.data.DataLoader(list(zip(val_X, val_y)), batch_size=128, drop_last=True)\nmodel_test = CNN_ForecastNet_pooled(input_size=num_features,len_timestamp =NUM_READINGS_PER_INTERVAL, hidden_size=64, output_size=len(labels_to_distance), kernel_size=3)\nmodel_test.load_state_dict(torch.load(MODEL_PATH))\n#print(sum(p.numel() for p in model_test.parameters()))\noutput_predictions(model_test, val_X, labels_to_distance, dev_interval_to_file, \"./NIST_{}_layers_2_coarse.tsv\".format(EXP_NAME))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:40:17.711551Z","iopub.execute_input":"2021-11-03T11:40:17.711987Z","iopub.status.idle":"2021-11-03T11:40:19.561764Z","shell.execute_reply.started":"2021-11-03T11:40:17.71195Z","shell.execute_reply":"2021-11-03T11:40:19.561017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./NIST_CNN_ForecastNet_pooled_layers_2_final_coarse_output.tsv\"> Download File </a>\n<a href=\"./model_epochs_100_CNN_ForecastNet_pooled_coarse.pth\"> Download File </a>\n<a href=\"./NIST_CNN_ForecastNet_pooled_layers_2_coarse.tsv\"> Download File </a>\n","metadata":{"execution":{"iopub.status.busy":"2021-10-17T09:34:15.375987Z","iopub.execute_input":"2021-10-17T09:34:15.376307Z","iopub.status.idle":"2021-10-17T09:34:15.382867Z","shell.execute_reply.started":"2021-10-17T09:34:15.376273Z","shell.execute_reply":"2021-10-17T09:34:15.381855Z"}}},{"cell_type":"code","source":"### EXP_NAME = \"CNN_ForecastNet_pooled\"\nMODEL_PATH = \"./model_epochs_100_{}.pth\".format(EXP_NAME)\nlabels_to_distance = [1.2, 3.0, 4.5,1.8]\nnum_features = 55\nbest_acc=0\n\n#val_data_loader = DataLoader(list(zip(val_X, val_y)), batch_size=50, drop_last=True)\nmodel_test = CNN_ForecastNet_pooled(input_size=num_features,len_timestamp =NUM_READINGS_PER_INTERVAL, hidden_size=64, output_size=len(labels_to_distance), kernel_size=3)\nmodel_test.load_state_dict(torch.load(MODEL_PATH))\n#print(len(test_X))\n#print(model_test)\n#print(sum(p.numel() for p in model_test.parameters()))\noutput_predictions(model_test, test_X, labels_to_distance, test_intervals_to_file , \"./NIST_{}_layers_2_final_coarse_output.tsv\".format(EXP_NAME))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T11:59:10.585201Z","iopub.execute_input":"2021-11-03T11:59:10.585895Z","iopub.status.idle":"2021-11-03T11:59:27.166195Z","shell.execute_reply.started":"2021-11-03T11:59:10.585856Z","shell.execute_reply":"2021-11-03T11:59:27.165482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}